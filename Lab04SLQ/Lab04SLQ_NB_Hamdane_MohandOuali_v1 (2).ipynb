{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPiZDPuuddDm"
      },
      "source": [
        "# 2CS SIL2-SIQ2 Lab04. Naïve Bayes\n",
        "\n",
        "<p style='text-align: right;font-style: italic;'>Designed by: Dr. Abdelkrime Aries</p>\n",
        "\n",
        "In this lab, we will learn about two generative models:\n",
        "- Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b90ANxKiddEG"
      },
      "source": [
        "**Team:**\n",
        "- **Member 01**: Hamdane Zakaria\n",
        "- **Member 02**: Mohand ouali manel\n",
        "- **Group**: SIL2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L6R-1P9PddEH",
        "outputId": "4cde58f0-142c-452b-def6-31f9ba5b6edb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import sys, timeit\n",
        "from typing          import Tuple, List, Type\n",
        "from collections.abc import Callable\n",
        "\n",
        "sys.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtzexj9ZddEJ",
        "outputId": "1a540520-16e7-45ff-bfb1-75180fd4f12f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.25.2', '2.0.3', '3.7.1')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy             as np\n",
        "import pandas            as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "np.__version__, pd.__version__, matplotlib.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B-sAsyr6ddEJ",
        "outputId": "62d9d184-d026-49d5-b326-e0a210f4940c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import sklearn\n",
        "\n",
        "from sklearn.naive_bayes   import CategoricalNB\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.metrics       import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection         import train_test_split\n",
        "from sklearn.naive_bayes             import MultinomialNB, GaussianNB\n",
        "from sklearn.linear_model            import LogisticRegression\n",
        "from sklearn.tree                    import DecisionTreeClassifier\n",
        "from sklearn.metrics                 import precision_score, recall_score\n",
        "import timeit\n",
        "\n",
        "\n",
        "sklearn.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYzH-XEGddEK"
      },
      "source": [
        "## I. Algorithms implementation\n",
        "\n",
        "In this section, we will try to implement multinomial Naive Bayes.\n",
        "\n",
        "\n",
        "**>> Try to use \"numpy\" which will save a lot of time and effort**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6Hkb7LwddEK",
        "outputId": "5cd0c4de-e439-4759-cdaf-74d303040791"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Dataset play\n",
        "\n",
        "# outlook & temperature & humidity & windy\n",
        "Xplay = np.array([\n",
        "    ['sunny'   , 'hot' , 'high'  , 'no'],\n",
        "    ['sunny'   , 'hot' , 'high'  , 'yes'],\n",
        "    ['overcast', 'hot' , 'high'  , 'no'],\n",
        "    ['rainy'   , 'mild', 'high'  , 'no'],\n",
        "    ['rainy'   , 'cool', 'normal', 'no'],\n",
        "    ['rainy'   , 'cool', 'normal', 'yes'],\n",
        "    ['overcast', 'cool', 'normal', 'yes'],\n",
        "    ['sunny'   , 'mild', 'high'  , 'no'],\n",
        "    ['sunny'   , 'cool', 'normal', 'no'],\n",
        "    ['rainy'   , 'mild', 'normal', 'no'],\n",
        "    ['sunny'   , 'mild', 'normal', 'yes'],\n",
        "    ['overcast', 'mild', 'high'  , 'yes'],\n",
        "    ['overcast', 'hot' , 'normal', 'no'],\n",
        "    ['rainy'   , 'mild', 'high'  , 'yes']\n",
        "])\n",
        "\n",
        "Yplay = np.array([\n",
        "    'no',\n",
        "    'no',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'no',\n",
        "    'yes',\n",
        "    'no',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'yes',\n",
        "    'no'\n",
        "])\n",
        "\n",
        "len(Xplay), len(Yplay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XnPIqYLddEK"
      },
      "source": [
        "### I.1. Prior probability\n",
        "\n",
        "Given an output list $Y$, the probability of each class $c_k$ is estimated as:\n",
        "$$p(c_k) = \\frac{|\\{y / y \\in Y \\text{ et } y = c_k\\}|}{|Y|}$$\n",
        "\n",
        "Our function must return two lists:\n",
        "- One containing the names of unique classes.\n",
        "- Another containing probabilities of unique classes of the first list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNtDQciWddEL",
        "outputId": "f71642c5-3910-493f-9438-f9ca230dfcbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['no', 'yes'], dtype='<U3'), array([-1.02961942, -0.44183275]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# TODO: Prior probability\n",
        "def fit_prior(Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    cls  = np.unique(Y) # vocabulary\n",
        "    prior = []\n",
        "    for c in cls:\n",
        "        count_c = np.sum(Y == c)\n",
        "        prior.append(np.log(count_c / len(Y)))\n",
        "    return cls, np.array(prior)\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# (array(['no', 'yes'], dtype='<U3'), array([-1.02961942, -0.44183275]))\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "fit_prior(Yplay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t3FCAtJddEL"
      },
      "source": [
        "### I.2. Multinomial Likelihood probability\n",
        "\n",
        "Given:\n",
        "- $A$: a categorical feature\n",
        "- $V$: unique values of this feature (feature's categories)\n",
        "- $Y$: the ouput\n",
        "- $C$: the classes\n",
        "- $\\alpha$: smoothing factor\n",
        "\n",
        "calculate the log likelihood:\n",
        "$$ \\log p(A=v|Y=c_k) = \\log(|\\{ y \\in Y / y = c_k \\text{ and } A = v\\}| + \\alpha) - \\log(|\\{y = c_k\\}| + \\alpha * |V|)$$\n",
        "\n",
        "The function must:\n",
        "- add a token \"\\<UNK\\>\" to $V$\n",
        "- return feature's categories (vocabulary) $V$\n",
        "- return a matrix where rows are $V$ and colums are $C$ containing the log probability $p(V|C)$\n",
        "- when alpha=0 and v=UNK, we will have an error, in this case we will consider log probability as $-\\infty$\n",
        "- when the probability is 0, we will consider log probability as $-\\infty$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n06hZhfyddEL",
        "outputId": "83912938-0cae-4054-b812-5e88046188f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-16fc7d611a8a>:30: RuntimeWarning: divide by zero encountered in log\n",
            "  log_probs = np.log(smoothed_counts) - np.log(smoothed_total)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'),\n",
              "  array([[       -inf, -0.81093022],\n",
              "         [-0.91629073, -1.09861229],\n",
              "         [-0.51082562, -1.5040774 ],\n",
              "         [       -inf,        -inf]])),\n",
              " (array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'),\n",
              "  array([[-2.19722458, -0.95551145],\n",
              "         [-1.09861229, -1.178655  ],\n",
              "         [-0.81093022, -1.46633707],\n",
              "         [-2.19722458, -2.56494936]])))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# TODO: Multinomial Likelihood probability\n",
        "def fit_likelihood(A: np.ndarray,\n",
        "                     Y: np.ndarray,\n",
        "                     C: np.ndarray,\n",
        "                     alpha: float = 0.\n",
        "                         ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    likelihood = []\n",
        "    V = np.unique(A)\n",
        "    V = np.concatenate([V, ['<UNK>']])\n",
        "    likelihood = np.zeros((len(V), len(C)))\n",
        "\n",
        "    # Iterate through unique categories in V\n",
        "    for i, v in enumerate(V):\n",
        "        # Initialize array to store counts for each class\n",
        "        counts_per_class = np.zeros(len(C))\n",
        "\n",
        "        # Iterate through classes\n",
        "        for j, c in enumerate(C):\n",
        "            count_A_and_Y = np.sum((A == v) & (Y == c))\n",
        "            counts_per_class[j] = count_A_and_Y\n",
        "\n",
        "        # Calculate denominator (total counts per class)\n",
        "        total_counts = np.sum(Y == C[:, None], axis=1)\n",
        "\n",
        "        # Apply smoothing\n",
        "        smoothed_counts = counts_per_class + alpha\n",
        "        smoothed_total = total_counts + alpha * len(V)\n",
        "\n",
        "        # Calculate log likelihood\n",
        "        log_probs = np.log(smoothed_counts) - np.log(smoothed_total)\n",
        "\n",
        "        # Avoid log(0) by setting probability to -inf for counts of 0\n",
        "        log_probs[np.isnan(log_probs)] = -np.inf\n",
        "\n",
        "        likelihood[i, :] = log_probs\n",
        "\n",
        "    return V, likelihood\n",
        "\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# ((array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'),\n",
        "#   array([[ -inf, -0.81093022],\n",
        "#         [-0.91629073, -1.09861229],\n",
        "#         [-0.51082562, -1.5040774 ],\n",
        "#         [ -inf, -inf]])),\n",
        "# (array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'),\n",
        "#  array([[-2.19722458, -0.95551145],\n",
        "#         [-1.09861229, -1.178655 ],\n",
        "#         [-0.81093022, -1.46633707],\n",
        "#         [-2.19722458, -2.56494936]])))\n",
        "#---------------------------------------------------------------------\n",
        "C_t = np.array(['no', 'yes'])\n",
        "fit_likelihood(Xplay[:, 0], Yplay, C_t), fit_likelihood(Xplay[:, 0], Yplay, C_t, alpha=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZFsT8BQddEM"
      },
      "source": [
        "### I.3. Model training\n",
        "\n",
        "**Nothing to code here, although you have to know how it functions for next use**\n",
        "\n",
        "Given a feature $X_j$, a value $v \\in X_j$ and a class $c_k$, the likeelihood is calculated:\n",
        "$$ P(X_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ and } X_j = v\\}|}{|\\{y = c_k\\}|}$$\n",
        "\n",
        "This function aims to generate parameters $\\theta$.\n",
        "In our case, paramters are diffrent from those of *logistic regrssion*.\n",
        "They are a dictionary (map) with two entries:\n",
        "- \"prior\" key having a dictionary as value, having \"v\" a list of values and \"p\" a list of their respective probabilities.\n",
        "- \"likelihood\" a list of dictinaries reprsnting statistics of each feature (the same order of $X$ features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owad10XQddEM",
        "outputId": "cf18a4de-8146-472e-e4ac-8e73a416d7e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prior': {'v': array(['no', 'yes'], dtype='<U3'),\n",
              "  'p': array([-1.02961942, -0.44183275])},\n",
              " 'likelihood': [{'v': array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'),\n",
              "   'p': array([[-2.19722458, -0.95551145],\n",
              "          [-1.09861229, -1.178655  ],\n",
              "          [-0.81093022, -1.46633707],\n",
              "          [-2.19722458, -2.56494936]])},\n",
              "  {'v': array(['cool', 'hot', 'mild', '<UNK>'], dtype='<U8'),\n",
              "   'p': array([[-1.5040774 , -1.178655  ],\n",
              "          [-1.09861229, -1.46633707],\n",
              "          [-1.09861229, -0.95551145],\n",
              "          [-2.19722458, -2.56494936]])},\n",
              "  {'v': array(['high', 'normal', '<UNK>'], dtype='<U8'),\n",
              "   'p': array([[-0.47000363, -1.09861229],\n",
              "          [-1.38629436, -0.5389965 ],\n",
              "          [-2.07944154, -2.48490665]])},\n",
              "  {'v': array(['no', 'yes', '<UNK>'], dtype='<U8'),\n",
              "   'p': array([[-0.98082925, -0.5389965 ],\n",
              "          [-0.69314718, -1.09861229],\n",
              "          [-2.07944154, -2.48490665]])}]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def fit_NB(X: np.ndarray,\n",
        "        Y: np.ndarray,\n",
        "        alpha=0.\n",
        "        ) -> object:\n",
        "\n",
        "    Theta   = {'prior': {}, 'likelihood': []}\n",
        "\n",
        "    Theta['prior']['v'], Theta['prior']['p'] = fit_prior(Y)\n",
        "\n",
        "    for j in range(X.shape[1]):\n",
        "        likelihood = {}\n",
        "        likelihood['v'], likelihood['p'] = fit_likelihood(X[:, j], Y, Theta['prior']['v'], alpha=alpha)\n",
        "        Theta['likelihood'].append(likelihood)\n",
        "\n",
        "    return Theta\n",
        "\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# {'prior': {'v': array(['no', 'yes'], dtype='<U3'),\n",
        "#    'p': array([-1.02961942, -0.44183275])},\n",
        "#    'likelihood': [{'v': array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'),\n",
        "#    'p': array([[-2.19722458, -0.95551145],\n",
        "#                     [-1.09861229, -1.178655 ],\n",
        "#                     [-0.81093022, -1.46633707],\n",
        "#                     [-2.19722458, -2.56494936]])},\n",
        "#     {'v': array(['cool', 'hot', 'mild', '<UNK>'], dtype='<U8'),\n",
        "#       'p': array([[-1.5040774 , -1.178655 ],\n",
        "#                        [-1.09861229, -1.46633707],\n",
        "#                        [-1.09861229, -0.95551145],\n",
        "#                        [-2.19722458, -2.56494936]])},\n",
        "#     {'v': array(['high', 'normal', '<UNK>'], dtype='<U8'),\n",
        "#       'p': array([[-0.47000363, -1.09861229],\n",
        "#                        [-1.38629436, -0.5389965 ],\n",
        "#                        [-2.07944154, -2.48490665]])},\n",
        "#     {'v': array(['no', 'yes', '<UNK>'], dtype='<U8'),\n",
        "#       'p': array([[-0.98082925, -0.5389965 ],\n",
        "#                        [-0.69314718, -1.09861229],\n",
        "#                        [-2.07944154, -2.48490665]])}]}\n",
        "#---------------------------------------------------------------------\n",
        "Theta_play = fit_NB(Xplay, Yplay, alpha=1)\n",
        "\n",
        "Theta_play"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diUVXGa-ddEN"
      },
      "source": [
        "### I.4. Multinomial prediction\n",
        "Let's examine prediction equation:\n",
        "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
        "\n",
        "Our goal is to calculate approximate probabilities of all classes given a sample like indicated in the next equation:\n",
        "$$P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
        "\n",
        "- Given one sample $x$, we have to return a list of probabilities.\n",
        "- If prior=false, we must not consider prior probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reIuKswbddEN",
        "outputId": "4b638d6b-28a7-48b3-c1de-af1d81959d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-6.81036293, -5.8230459 ]), array([-4.68213123, -3.99491878]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# You can use this function in the next implimentation\n",
        "# It takes a list of unique values V and a given value v\n",
        "# It returns the position of v in V\n",
        "# If v does not exist in V, it rturns -1\n",
        "def find_idx(V: np.ndarray, v: str) -> int:\n",
        "    k = np.argwhere(V == v).flatten()\n",
        "    if len(k):\n",
        "        return k[0]\n",
        "    return -1\n",
        "\n",
        "# TODO: Prediction\n",
        "def predict_NB_1(Xi: np.ndarray, Theta: dict, add_prior: bool = True) -> List[float]:\n",
        "    prior_v, prior_p = Theta['prior']['v'], Theta['prior']['p']\n",
        "    likelihoods = Theta['likelihood']\n",
        "    probabilities = []\n",
        "\n",
        "    for c in range(len(prior_v)):\n",
        "      p = prior_p[c] if add_prior else 0\n",
        "      for j, v in enumerate(Xi):\n",
        "        likelihood_v = likelihoods[j][\"v\"]\n",
        "        idx = find_idx(likelihood_v, v)\n",
        "        if idx != -1:\n",
        "          p += likelihoods[j]['p'][idx, c]\n",
        "        else :\n",
        "          p += likelihoods[j]['p'][find_idx(\"<UNK>\", v), c]\n",
        "\n",
        "      probabilities.append(p)\n",
        "\n",
        "    return np.array(probabilities)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# (array([-6.81036293, -5.8230459 ]), array([-4.68213123, -3.99491878]))\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "X_t = np.array([\n",
        "    ['rainy', 'cool', 'normal', 'yes'],\n",
        "    ['snowy', 'cool', 'normal', 'yes'],\n",
        "    ['sunny', 'hot' , 'normal', 'no']\n",
        "])\n",
        "\n",
        "predict_NB_1(X_t[1, :], Theta_play), predict_NB_1(X_t[0, :], Theta_play, add_prior=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onu3GB8AddEN"
      },
      "source": [
        "### I.5. Final product\n",
        "\n",
        "**>> Nothing to code here**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztJb5b_5ddEO",
        "outputId": "06029f08-79de-4cbe-9fb9-32a6a8bcd9b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['yes', 'yes', 'yes'], dtype='<U3'),\n",
              " array([[-5.71175064, -4.43675153],\n",
              "        [-6.81036293, -5.8230459 ],\n",
              "        [-5.30628554, -4.45249989]]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "class OurCategoricalNB(object):\n",
        "\n",
        "    def fit(self, X, Y, alpha=0.):\n",
        "        self.Theta = fit_NB(X, Y, alpha=alpha)\n",
        "\n",
        "    def predict(self, X, add_prior=True, prob=False):\n",
        "        Y_pred = []\n",
        "        for i in range(len(X)):\n",
        "            Y_pred.append(predict_NB_1(X[i,:], self.Theta, add_prior=add_prior))\n",
        "\n",
        "        Y_pred = np.array(Y_pred)\n",
        "\n",
        "        if prob:\n",
        "            return Y_pred\n",
        "\n",
        "        return np.choose(np.argmax(Y_pred, axis=1), self.Theta['prior']['v'])\n",
        "\n",
        "#=====================================================================\n",
        "# UNIT TEST\n",
        "#=====================================================================\n",
        "# Result:\n",
        "# (array(['yes', 'yes', 'yes'], dtype='<U3'),\n",
        "#     array([[-5.71175064, -4.43675153],\n",
        "#             [-6.81036293, -5.8230459 ],\n",
        "#             [-5.30628554, -4.45249989]]))\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "cnb = OurCategoricalNB()\n",
        "cnb.fit(Xplay, Yplay, alpha=1)\n",
        "X_t = np.array([\n",
        "    ['rainy', 'cool', 'normal', 'yes'],\n",
        "    ['snowy', 'cool', 'normal', 'yes'],\n",
        "    ['sunny', 'hot' , 'normal', 'no']\n",
        "])\n",
        "\n",
        "cnb.predict(X_t), cnb.predict(X_t, prob=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHeuLllMddEO"
      },
      "source": [
        "## II. Application and Analysis\n",
        "\n",
        "In this section, we will test different concepts by running an experiment, formulating a hypothesis and trying to justify it.\n",
        "\n",
        "### II.1. Prior probability\n",
        "\n",
        "We want to test the effect of prior probability.\n",
        "To do this, we trained two models:\n",
        "1. With prior probability\n",
        "1. Without prior probability (It considers a uniform distribution of classes)\n",
        "\n",
        "To test whether the models have adapted well to the training dataset, we will test them on the same dataset and calculate the classification ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9oEpFzoddEO",
        "outputId": "667c4bf4-897a-4b67-90b0-03302b544342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Considring prior probability\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       1.00      0.80      0.89         5\n",
            "         yes       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.95      0.90      0.92        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n",
            "No prior probability\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.67      0.80      0.73         5\n",
            "         yes       0.88      0.78      0.82         9\n",
            "\n",
            "    accuracy                           0.79        14\n",
            "   macro avg       0.77      0.79      0.78        14\n",
            "weighted avg       0.80      0.79      0.79        14\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nb_withPrior     = CategoricalNB(alpha=1.0, fit_prior=True )\n",
        "nb_noPrior       = CategoricalNB(alpha=1.0, fit_prior=False)\n",
        "\n",
        "enc         = OrdinalEncoder()\n",
        "Xplay_tf    = enc.fit_transform(Xplay)\n",
        "nb_withPrior.fit(Xplay_tf, Yplay)\n",
        "nb_noPrior.fit(Xplay_tf, Yplay)\n",
        "\n",
        "Ypred_withPrior = nb_withPrior.predict(Xplay_tf)\n",
        "Ypred_noPrior = nb_noPrior.predict(Xplay_tf)\n",
        "\n",
        "\n",
        "print( 'Considring prior probability'  )\n",
        "print(classification_report(Yplay, Ypred_withPrior))\n",
        "\n",
        "print( 'No prior probability'  )\n",
        "print(classification_report(Yplay, Ypred_noPrior))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr43xIvxddEP"
      },
      "source": [
        "**TODO: Analyze the results**\n",
        "\n",
        "1. What do you notice, indicating if prior probability is useful in this case?\n",
        "1. How does this probability affect the outcome?\n",
        "1. When are we sure that using this probability is unnecessary?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "1. we observe that all measurement metrics decrease when the prior probability is not taken into account, compared to when it is considered. Yes, the prior probability is important in this case, as long as the dataset distribution is not uniform (the number of individuals belonging to class 1 is significantly different from class 2)\n",
        "1. incorporating this probability (when dealing with imbalanced classes) enhances model performance by increasing metrics like precision. This is because it accounts for the distribution of these classes within the dataset.\n",
        "1. when we are confident that the class distribution is uniform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E7mGYuhddEP"
      },
      "source": [
        "### II.2. Smoothing\n",
        "\n",
        "We want to test the Lidstone smoothing's effect.\n",
        "To do this, we trained three models:\n",
        "1. alpha = 1 (Laplace smoothing)\n",
        "1. alpha = 0.5\n",
        "1. alpha = 0 (without smoothing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj6u4JzvddEP",
        "outputId": "673a9ed2-5c09-4dd5-c0d7-e33a8200db9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha = 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       1.00      0.80      0.89         5\n",
            "         yes       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.95      0.90      0.92        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n",
            "Alpha = 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       1.00      0.80      0.89         5\n",
            "         yes       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.95      0.90      0.92        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n",
            "Alpha = 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       1.00      0.80      0.89         5\n",
            "         yes       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.95      0.90      0.92        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py:629: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py:635: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "NBC_10 = CategoricalNB(alpha = 1.0 )\n",
        "NBC_05 = CategoricalNB(alpha = 0.5 )\n",
        "NBC_00 = CategoricalNB(alpha = 0.0 )\n",
        "\n",
        "NBC_10.fit( Xplay_tf,   Yplay )\n",
        "NBC_05.fit( Xplay_tf,   Yplay )\n",
        "NBC_00.fit( Xplay_tf,   Yplay )\n",
        "\n",
        "Y_10   = NBC_10.predict(Xplay_tf)\n",
        "Y_05   = NBC_05.predict(Xplay_tf)\n",
        "Y_00   = NBC_00.predict(Xplay_tf)\n",
        "\n",
        "\n",
        "print(          'Alpha = 1.0'             )\n",
        "print(classification_report(Yplay, Y_10))\n",
        "\n",
        "print(          'Alpha = 0.5'             )\n",
        "print(classification_report(Yplay, Y_05))\n",
        "\n",
        "print(          'Alpha = 0.0'             )\n",
        "print(classification_report(Yplay, Y_00))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzjIDP2nddEP"
      },
      "source": [
        "**TODO: Analyze the results**\n",
        "\n",
        "1. What do you notice, indicating if smoothing affects performance in this case?\n",
        "1. Based on the past answeer, Why?\n",
        "1. Why do we get a \"RuntimeWarning: divide by zero\" error?\n",
        "1. What is the benefit of smoothing (generally; not just for this case)?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "1. .We notice that for all three alpha values, we obtain the same values for all metrics.No, smoothing does not affect performance in this case\n",
        "1. smoothing does not affect performance in this case because all test dataset values exist in the training dataset, so we do not encounter the case where P(Xi=v/Y=Ck) equals 0.\n",
        "1. The \"RuntimeWarning: divide by zero\" occurs due to attempting to compute the logarithm of zero during the Naive Bayes algorithm. It happens when a feature count in a class is zero, and Laplace smoothing (alpha > 0) is applied, resulting in a division by zero. This warning indicates that such cases were encountered during computation.\n",
        "1. smoothing is primarily used to prevent probabilities of class membership from being zero in cases where we have zero likelihood probabilities with the Naive Bayes classification model. By adding the alpha parameter for all feature values, we maintain the same model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl9kilZzddEP"
      },
      "source": [
        "### II.3. Naive Bayes performance\n",
        "\n",
        "Naive Bayes is known to generate powerful models when it comes to classifying textual documents.\n",
        "We want to test this proposition using spam detection over [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) dataset.\n",
        "\n",
        "Each message is represented using term frequency (TF), where a word is considered as a feature.\n",
        "In this case, a message is represented by a vector of frequencies (how many times each word appeared in the message).\n",
        "We want to compare these models:\n",
        "1. Multinomial Naive Bayes (MNB)\n",
        "1. Gaussian Naive Bayes (GNB)\n",
        "1. Logistic Regression (LR)\n",
        "1. Decision Tree (DT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uip4m5lddEQ",
        "outputId": "a3fde9f5-b74e-4099-fa87-edf166d64bb5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text class\n",
              "0  Go until jurong point, crazy.. Available only ...   ham\n",
              "1                      Ok lar... Joking wif u oni...   ham\n",
              "2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n",
              "3  U dun say so early hor... U c already then say...   ham\n",
              "4  Nah I don't think he goes to usf, he lives aro...   ham"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading the dataset\n",
        "messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
        "# renaming features: text and class\n",
        "messages = messages.rename(columns={'v1': 'class', 'v2': 'text'})\n",
        "# keeping only these two features\n",
        "messages = messages.filter(['text', 'class'])\n",
        "\n",
        "messages.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGSa1niMddEQ",
        "outputId": "406d61d3-7ca3-4b89-d229-c16c57c0f113"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Algorithm</th>\n",
              "      <th>Train time</th>\n",
              "      <th>Test time</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multinomial Naive Bayes (MNB)</td>\n",
              "      <td>0.559776</td>\n",
              "      <td>0.029081</td>\n",
              "      <td>0.987179</td>\n",
              "      <td>0.927711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gaussian Naive Bayes  (GNB)</td>\n",
              "      <td>0.551694</td>\n",
              "      <td>0.127377</td>\n",
              "      <td>0.616667</td>\n",
              "      <td>0.891566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logistic Regression (LR)</td>\n",
              "      <td>0.555998</td>\n",
              "      <td>0.028857</td>\n",
              "      <td>0.986111</td>\n",
              "      <td>0.855422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decision Tree (DT)</td>\n",
              "      <td>13.080956</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>0.909677</td>\n",
              "      <td>0.849398</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Algorithm  Train time  Test time  Precision    Recall\n",
              "0  Multinomial Naive Bayes (MNB)    0.559776   0.029081   0.987179  0.927711\n",
              "1    Gaussian Naive Bayes  (GNB)    0.551694   0.127377   0.616667  0.891566\n",
              "2       Logistic Regression (LR)    0.555998   0.028857   0.986111  0.855422\n",
              "3             Decision Tree (DT)   13.080956   0.016129   0.909677  0.849398"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models = [\n",
        "    MultinomialNB(),\n",
        "    GaussianNB(),\n",
        "    LogisticRegression(solver='lbfgs'),\n",
        "    #solver=sag is slower; so I chose the fastest\n",
        "    DecisionTreeClassifier()\n",
        "]\n",
        "\n",
        "algos = [\n",
        "    'Multinomial Naive Bayes (MNB)',\n",
        "    'Gaussian Naive Bayes  (GNB)',\n",
        "    'Logistic Regression (LR)',\n",
        "    'Decision Tree (DT)'\n",
        "]\n",
        "\n",
        "perf = {\n",
        "    'train_time': [],\n",
        "    'test_time' : [],\n",
        "    'recall'    : [],\n",
        "    'precision' : []\n",
        "}\n",
        "\n",
        "\n",
        "msg_train, msg_test, Y_train, Y_test = train_test_split(messages['text'] ,\n",
        "                                                        messages['class'],\n",
        "                                                        test_size    = 0.2,\n",
        "                                                        random_state = 0  )\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_train          = count_vectorizer.fit_transform(msg_train).toarray()\n",
        "X_test           = count_vectorizer.transform    (msg_test ).toarray()\n",
        "\n",
        "\n",
        "for model in models:\n",
        "    # ==================================\n",
        "    # TRAIN\n",
        "    # ==================================\n",
        "    start_time = timeit.default_timer()\n",
        "    model.fit(X_train, Y_train)\n",
        "    perf['train_time'].append(timeit.default_timer() - start_time)\n",
        "\n",
        "    # ==================================\n",
        "    # TEST\n",
        "    # ==================================\n",
        "    start_time = timeit.default_timer()\n",
        "    Y_pred     = model.predict(X_test)\n",
        "    perf['test_time'].append(timeit.default_timer() - start_time)\n",
        "\n",
        "    # ==================================\n",
        "    # PERFORMANCE\n",
        "    # ==================================\n",
        "    # In here, we are interrested in \"spam\" class which is our positive class\n",
        "    perf['precision'].append(precision_score(Y_test, Y_pred, pos_label='spam'))\n",
        "    perf['recall'   ].append(recall_score   (Y_test, Y_pred, pos_label='spam'))\n",
        "\n",
        "\n",
        "pd.DataFrame({\n",
        "    'Algorithm' : algos,\n",
        "    'Train time': perf['train_time'],\n",
        "    'Test time' : perf['test_time'],\n",
        "    'Precision' : perf['precision'],\n",
        "    'Recall'    : perf['recall']\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zfJJmXddEQ"
      },
      "source": [
        "**TODO: Analyze the results**\n",
        "\n",
        "1. What do you notice about training time? (order the algorithms)\n",
        "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to training time)\n",
        "1. What do you notice about the testing time? (order the algorithms)\n",
        "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to testing time)\n",
        "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the two algorithms?\n",
        "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the problem/data?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "1. we notice that the algorithm taking the longest training time is Decision Tree, followed by Multinomial Naive Bayes,then Logistic Regression,  while the fastest training time is observed with Gaussian Naive Bayess\n",
        "1.let's discuss each algorithm:     \n",
        "    1-Decision Tree (DT): It's computationally intensive because it recursively splits the data based on features, aiming to fit the training data perfectly, which can lead to overfitting. This tends to slow down the process.\n",
        "\n",
        "    2-Multinomial Naive Bayes (MNB): It computes probabilities based on simple frequency counts of words in documents which makes. A simpler prrocess which leads to faster training times than DT.\n",
        "\n",
        "    3-Logistic Regression (LR): It's moderately fast, benefiting from convex optimization techniques.It's generally faster than DT because it involves optimizing a convex cost function, which converges faster.\n",
        "\n",
        "    4-Gaussian Naive Bayes (GNB): It's fast due to its simplistic assumptions about the distribution of data, making it computationally efficient.\n",
        "\n",
        "1. we observe that the algorithm that takes the longest testing time is Gaussian Naive Bayes, followed by Multinomial Naive Bayes, then Logistic Regression. The fastest training time is observed with Decision Trees\n",
        "1. let's discuss the algorithms:\n",
        "\n",
        "    1-Gaussian Naive Bayes (GNB): Testing is slower because it computes probabilities based on the Gaussian distribution assumption, which requires more computation, especially for each test instance.\n",
        "\n",
        "    2-Multinomial Naive Bayes (MNB): Is faster than GNB as it calculates probabilities based on simple term frequencies.It still requires computation for each test instance though, which can make it slower than other algorithms.\n",
        "\n",
        "    3-Logistic Regression (LR): Testing involves computing the dot product of the feature vector with learned weights and applying the sigmoid function. It's generally faster than Naive Bayes, but still requires some computation.\n",
        "\n",
        "    4-Decision Tree (DT): Testing is usually the fastest as it involves traversing the pre-built tree for predictions. This operation can prove to be fast compared to other computational methods.\n",
        "\n",
        "1. the Gaussian model assumes a Gaussian distribution of features, which may not be suitable for all types of data, especially in the case of textual data like spam detection. The multinomial model, on the other hand, is specifically designed for discrete data such as word counts, making it more appropriate for this type of problem\n",
        "1. the Gaussian Naive Bayes model is less suitable for classification with discrete and asymmetric data, such as text data, due to its assumption of continuous and symmetric data distribution. In contrast, the multinomial Naive Bayes model is specifically designed for discrete data, making it more effective in such cases. Therefore, when dealing with discrete data like text, the multinomial Naive Bayes model outperforms the Gaussian Naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WINWF7FbddEQ",
        "outputId": "ef8ea5ef-96be-45b4-cfbd-4dd879480c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  _____    __                                              _               \n",
            " |_   _|  / _|                                            | |              \n",
            "   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \n",
            "   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \n",
            "  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \n",
            " |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \n",
            "                   __/ |                     __/ |                         \n",
            "                  |___/                     |___/                          \n",
            "  _     _       _            __                                            \n",
            " | |   | |     (_)          / _|                 _                         \n",
            " | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \n",
            " | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \n",
            " | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \n",
            "  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \n",
            "                                                |/                         \n",
            "                                                                           \n",
            "                                                                           \n",
            "                                                                           \n",
            "  _   _    ___    _   _      __ _   _ __    ___                            \n",
            " | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \n",
            " | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \n",
            "  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \n",
            "   __/ |                                                                   \n",
            "  |___/                                                                    \n",
            "                    _                                                __    \n",
            "                   | |                                            _  \\ \\   \n",
            "  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \n",
            " | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \n",
            " | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \n",
            " |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \n",
            "                                                                     /_/   \n",
            "                                                                           \n"
          ]
        }
      ],
      "source": [
        "print(\"  _____    __                                              _               \")\n",
        "print(\" |_   _|  / _|                                            | |              \")\n",
        "print(\"   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \")\n",
        "print(\"   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \")\n",
        "print(\"  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \")\n",
        "print(\" |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \")\n",
        "print(\"                   __/ |                     __/ |                         \")\n",
        "print(\"                  |___/                     |___/                          \")\n",
        "print(\"  _     _       _            __                                            \")\n",
        "print(\" | |   | |     (_)          / _|                 _                         \")\n",
        "print(\" | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \")\n",
        "print(\" | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \")\n",
        "print(\" | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \")\n",
        "print(\"  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \")\n",
        "print(\"                                                |/                         \")\n",
        "print(\"                                                                           \")\n",
        "print(\"                                                                           \")\n",
        "print(\"                                                                           \")\n",
        "print(\"  _   _    ___    _   _      __ _   _ __    ___                            \")\n",
        "print(\" | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \")\n",
        "print(\" | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \")\n",
        "print(\"  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \")\n",
        "print(\"   __/ |                                                                   \")\n",
        "print(\"  |___/                                                                    \")\n",
        "print(\"                    _                                                __    \")\n",
        "print(\"                   | |                                            _  \\ \\   \")\n",
        "print(\"  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \")\n",
        "print(\" | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \")\n",
        "print(\" | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \")\n",
        "print(\" |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \")\n",
        "print(\"                                                                     /_/   \")\n",
        "print(\"                                                                           \")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}